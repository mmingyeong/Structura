#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
compute_densitymap_fft.py

This script reads .npy files from a configuration-specified directory,
performs FFT-based kernel density estimation using the CPU-only version 
of the FFTKDE class, and saves the resulting density maps to an output directory.
Parallel processing is implemented via ProcessPoolExecutor using CPU cores only,
and Dask is used for out-of-core processing in FFTKDE.
A progress bar is displayed to show the processing status.

Author: Mingyeong Yang (mmingyeong@kasi.re.kr)
Date: 2025-03-07 (revised with Dask distributed client, chunk processing, and memory mapping)
"""

import os
import sys
import time
import logging
import numpy as np
import psutil
import multiprocessing
from tqdm import tqdm
from functools import partial
from concurrent.futures import ProcessPoolExecutor
from dask.distributed import Client

# -------------------------------
# ê²½ë¡œ ì„¤ì • ë° ëª¨ë“ˆ ì„í¬íŠ¸
# -------------------------------
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))

from config import OUTPUT_DATA_PATHS
from fft_kde import FFTKDE
from kernel import KernelFunctions

npy_folder = OUTPUT_DATA_PATHS["TNG300_snapshot99"]

# -------------------------------
# ë¡œê¹… ì„¤ì •
# -------------------------------
logger = logging.getLogger("FFTKDELogger")
logger.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

ch = logging.StreamHandler()
ch.setLevel(logging.DEBUG)
ch.setFormatter(formatter)
logger.addHandler(ch)

fh = logging.FileHandler("fft_kde.log", mode="a")
fh.setLevel(logging.DEBUG)
fh.setFormatter(formatter)
logger.addHandler(fh)

# -------------------------------
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê·¸ í•¨ìˆ˜
# -------------------------------
def log_memory_usage(label=""):
    process = psutil.Process(os.getpid())
    mem_mb = process.memory_info().rss / 1024**2
    logger.info(f"ğŸ’¾ Memory usage {label}: {mem_mb:.2f} MB")

# -------------------------------
# íŒŒì¼ë³„ ë°€ë„ ê³„ì‚° í•¨ìˆ˜ (FFTKDE.compute_density() í˜¸ì¶œ)
# -------------------------------
def compute_density_for_file(npy_path: str, output_dir: str,
                              cube_size: float = 205.0,
                              bandwidth: float = 1.0,
                              grid_resolution: float = 0.82,
                              kernel=KernelFunctions.uniform):
    try:
        # ë©”ëª¨ë¦¬ ë§¤í•‘ ë°©ì‹ìœ¼ë¡œ ì…ë ¥ íŒŒì¼ ë¡œë“œ
        particles = np.load(npy_path, mmap_mode='r')
    except Exception as e:
        logger.error("âŒ Failed to load file %s: %s", npy_path, str(e))
        return

    base_name = os.path.splitext(os.path.basename(npy_path))[0]
    output_file = os.path.join(output_dir, f"density_fft_{base_name}_{kernel.__name__}.npy")
    if os.path.exists(output_file):
        logger.warning("âš ï¸ Output already exists: %s. Skipping.", output_file)
        return

    grid_bounds = {'x': (0.0, cube_size),
                   'y': (0.0, cube_size),
                   'z': (0.0, cube_size)}
    grid_spacing = (grid_resolution, grid_resolution, grid_resolution)

    logger.info("ğŸ“Š Starting FFTKDE for [%s] (N=%d)", base_name, particles.shape[0])
    log_memory_usage("before FFTKDE compute")

    try:
        fft_kde = FFTKDE(particles, grid_bounds, grid_spacing,
                         kernel_func=kernel, h=bandwidth)
        _, _, _, density_map = fft_kde.compute_density()
    except Exception as e:
        logger.error("âŒ Error during density computation [%s]: %s", base_name, str(e))
        return

    log_memory_usage("after FFTKDE compute")

    try:
        # ë©”ëª¨ë¦¬ ë§¤í•‘ì„ í™œìš©í•´ ìµœì¢… ê²°ê³¼ ì €ì¥
        from numpy.lib.format import open_memmap
        fp = open_memmap(output_file, mode='w+', dtype=density_map.dtype, shape=density_map.shape)
        fp[:] = density_map[:]
    except Exception as e:
        logger.error("âŒ Error saving density map for file %s: %s", base_name, str(e))
        return

    logger.info("âœ… Density map saved: %s", output_file)
    print(f"[{base_name}] Density map saved to {output_file}")

# -------------------------------
# íŒŒì¼ ë‹¨ìœ„ ì²˜ë¦¬ í•¨ìˆ˜ (ê° íŒŒì¼ì— ëŒ€í•´ compute_density_for_file() í˜¸ì¶œ)
# -------------------------------
def process_file(fname, output_dir, cube_size, bandwidth, grid_resolution, kernel):
    full_path = os.path.join(npy_folder, fname)
    logger.info("ğŸ”¹ Processing file: %s", fname)
    compute_density_for_file(full_path, output_dir,
                             cube_size=cube_size,
                             bandwidth=bandwidth,
                             grid_resolution=grid_resolution,
                             kernel=kernel)

# -------------------------------
# ë©”ëª¨ë¦¬ ê¸°ë°˜ max_workers ê³„ì‚° í•¨ìˆ˜
# -------------------------------
def estimate_memory_usage(resolution: float, cube_size: float = 205.0) -> float:
    grid_points = int(cube_size / resolution)
    total_elements = grid_points ** 3
    bytes_needed = total_elements * np.dtype(np.complex128).itemsize
    return bytes_needed / 1024**3  # GiB

def determine_max_workers(resolution: float, buffer_gib: float = 30.0) -> int:
    available_ram_gib = psutil.virtual_memory().available / 1024**3 - buffer_gib
    mem_per_task = estimate_memory_usage(resolution)
    max_tasks = int(available_ram_gib // mem_per_task)
    return max(1, min(max_tasks, multiprocessing.cpu_count()))

# -------------------------------
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# -------------------------------
def main():
    # Dask Distributed Client ì´ˆê¸°í™” (ê° í”„ë¡œì„¸ìŠ¤ëŠ” ê°œë³„ì ìœ¼ë¡œ Dask ì‘ì—…ì„ ìˆ˜í–‰)
    try:
        from dask.distributed import Client
        client = Client()  # ê¸°ë³¸ í´ëŸ¬ìŠ¤í„°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.
        logger.info("Dask distributed client started. Dashboard: %s", client.dashboard_link)
    except Exception as e:
        logger.error("Failed to start Dask distributed client: %s", str(e))

    # ì‚¬ìš©ì ì„¤ì •ê°’
    resolution = 0.41
    kernel = KernelFunctions.uniform
    cube_size = 205.0
    bandwidth = 1.0

    output_dir = f"fft_densitymap_{kernel.__name__}_dx{resolution}"
    os.makedirs(output_dir, exist_ok=True)

    all_files = sorted([f for f in os.listdir(npy_folder) if f.endswith(".npy")])
    logger.info("ğŸ“ Found %d input files in: %s", len(all_files), npy_folder)

    max_workers = determine_max_workers(resolution)
    logger.info("ğŸ§  Auto-selected max_workers: %d (resolution=%.2f)", max_workers, resolution)

    proc_file = partial(process_file,
                        output_dir=output_dir,
                        cube_size=cube_size,
                        bandwidth=bandwidth,
                        grid_resolution=resolution,
                        kernel=kernel)

    start_time = time.time()
    futures = []
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        for fname in all_files:
            futures.append(executor.submit(proc_file, fname))
        for f in tqdm(futures, desc="Processing files", total=len(futures)):
            try:
                f.result()
            except Exception as e:
                logger.error("âŒ Exception occurred: %s", str(e))

    elapsed = time.time() - start_time
    logger.info("ğŸ All files processed. Elapsed time: %.2f seconds", elapsed)
    print(f"âœ… Total execution time: {elapsed:.2f} seconds")

# -------------------------------
# ì‹¤í–‰ ì‹œì‘
# -------------------------------
if __name__ == "__main__":
    main()
